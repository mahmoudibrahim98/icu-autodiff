{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKFG-wkL06ed"
      },
      "source": [
        "# 0) Prepare Colab environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "b1UU89ym06ee",
        "outputId": "c8257b40-acc1-4185-9a90-cbb3d57319fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'icu-autodiff'...\n",
            "remote: Enumerating objects: 96, done.\u001b[K\n",
            "remote: Counting objects: 100% (96/96), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 96 (delta 43), reused 73 (delta 22), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (96/96), 457.90 KiB | 5.45 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n",
            "/content/icu-autodiff\n",
            "Requirement already satisfied: pandas<2.3.0,>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from -r colab-compatible-requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from -r colab-compatible-requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: torch<2.7.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from -r colab-compatible-requirements.txt (line 4)) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from -r colab-compatible-requirements.txt (line 5)) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from -r colab-compatible-requirements.txt (line 6)) (1.15.3)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from -r colab-compatible-requirements.txt (line 9)) (3.10.0)\n",
            "Requirement already satisfied: seaborn>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from -r colab-compatible-requirements.txt (line 10)) (0.13.2)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from -r colab-compatible-requirements.txt (line 11)) (5.24.1)\n",
            "Requirement already satisfied: tqdm>=4.60.0 in /usr/local/lib/python3.11/dist-packages (from -r colab-compatible-requirements.txt (line 14)) (4.67.1)\n",
            "Requirement already satisfied: rich<14.0.0,>=12.4.4 in /usr/local/lib/python3.11/dist-packages (from -r colab-compatible-requirements.txt (line 15)) (13.9.4)\n",
            "Collecting absl-py>=2.0.0 (from -r colab-compatible-requirements.txt (line 16))\n",
            "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: pyarrow<20.0.0,>=14.0.0 in /usr/local/lib/python3.11/dist-packages (from -r colab-compatible-requirements.txt (line 19)) (18.1.0)\n",
            "Requirement already satisfied: fsspec<2025.4.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from -r colab-compatible-requirements.txt (line 20)) (2025.3.2)\n",
            "Requirement already satisfied: packaging<25.0,>=23.2 in /usr/local/lib/python3.11/dist-packages (from -r colab-compatible-requirements.txt (line 23)) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=2.2.2->-r colab-compatible-requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=2.2.2->-r colab-compatible-requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=2.2.2->-r colab-compatible-requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4)) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4)) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.5.0->-r colab-compatible-requirements.txt (line 5)) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.5.0->-r colab-compatible-requirements.txt (line 5)) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r colab-compatible-requirements.txt (line 9)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r colab-compatible-requirements.txt (line 9)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r colab-compatible-requirements.txt (line 9)) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r colab-compatible-requirements.txt (line 9)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r colab-compatible-requirements.txt (line 9)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r colab-compatible-requirements.txt (line 9)) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->-r colab-compatible-requirements.txt (line 11)) (9.1.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=12.4.4->-r colab-compatible-requirements.txt (line 15)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=12.4.4->-r colab-compatible-requirements.txt (line 15)) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=12.4.4->-r colab-compatible-requirements.txt (line 15)) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=2.2.2->-r colab-compatible-requirements.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.7.0,>=2.6.0->-r colab-compatible-requirements.txt (line 4)) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, absl-py, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.4.0\n",
            "    Uninstalling absl-py-1.4.0:\n",
            "      Successfully uninstalled absl-py-1.4.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed absl-py-2.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# ðŸ“¦ 1) Clone your repo & cd into it\n",
        "# -------------------------------------------------------------------\n",
        "!git clone https://github.com/mahmoudibrahim98/icu-autodiff.git\n",
        "%cd icu-autodiff\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# ðŸ“¦ 2) Install  project dependencies\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "!pip install -r colab-compatible-requirements.txt\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# ðŸ“¦ 3) (If your code lives in subfolders) add them to PYTHONPATH\n",
        "# -------------------------------------------------------------------\n",
        "import sys\n",
        "sys.path.append('.')         # or 'src', etc., depending on your layout\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# ðŸ“¦ 4) (Optional) Mount Drive for large data\n",
        "# -------------------------------------------------------------------\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#DATA_DIR = '/content/drive/MyDrive/yourproject/data'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "\n"
      ],
      "metadata": {
        "id": "HB5d_pwz4rLB",
        "outputId": "ce288624-dc67-4a21-8841-f994b3c9f245",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import gdown\n",
        "\n",
        "os.mkdir('raw_data')\n",
        "!gdown --folder https://drive.google.com/drive/folders/1x0iEVuudDHcVaqb4HDD8R6Bp9vlIW8Pg?usp=sharing \\\n",
        "  -O ./raw_data\n",
        "\n"
      ],
      "metadata": {
        "id": "JsvxXLJ84n9f",
        "outputId": "d7c4f044-7770-4568-d7c7-d675d897c5c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Retrieving folder 1b06_mXLT4XlF4ydKVvA0KkStwuJppbGy mortality24\n",
            "Retrieving folder 1pPj1f27ExXcX4lwqLXiw7zmS9j70kjJ3 eicu\n",
            "Processing file 1o_iT-ryr3g9uK3WLNFKYBvgvejGqabQF attrition.csv\n",
            "Processing file 1KxLPlQ_qAC_8m8NAdsgfXLifMLLGlR_9 dyn.parquet\n",
            "Processing file 1J7CCV24JAMSCJ5cM1NXBxDfjFfFMoloG outc.parquet\n",
            "Processing file 1UQLHzubpCty3K5d7NFoSnWSeRmztslcx sta.parquet\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1o_iT-ryr3g9uK3WLNFKYBvgvejGqabQF\n",
            "To: /content/icu-autodiff/raw_data/mortality24/eicu/attrition.csv\n",
            "100% 429/429 [00:00<00:00, 1.46MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KxLPlQ_qAC_8m8NAdsgfXLifMLLGlR_9\n",
            "To: /content/icu-autodiff/raw_data/mortality24/eicu/dyn.parquet\n",
            "100% 33.4M/33.4M [00:00<00:00, 63.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1J7CCV24JAMSCJ5cM1NXBxDfjFfFMoloG\n",
            "To: /content/icu-autodiff/raw_data/mortality24/eicu/outc.parquet\n",
            "100% 719k/719k [00:00<00:00, 150MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UQLHzubpCty3K5d7NFoSnWSeRmztslcx\n",
            "To: /content/icu-autodiff/raw_data/mortality24/eicu/sta.parquet\n",
            "100% 1.72M/1.72M [00:00<00:00, 128MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5YKVafi06ee"
      },
      "source": [
        "# 1) Data Preparation (Needed once)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "v7wRjaUQ06ef",
        "outputId": "e0768543-8636-4f20-c8c0-c6a1764b0f1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current CUDA device: 0\n",
            "True\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'data_access'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f6f5d1138b07>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdata_access\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_loader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbase_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_access\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mricu_loader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mricu_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mabsl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_access'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\n",
        "import sys\n",
        "import os\n",
        "# Add the project root directory to the Python path\n",
        "parent_dir = os.path.dirname(os.path.abspath(''))\n",
        "sys.path.append(parent_dir)\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Set the CUDA device to 0\n",
        "torch.cuda.set_device(0)\n",
        "\n",
        "# Verify the current device\n",
        "current_device = torch.cuda.current_device()\n",
        "print(f\"Current CUDA device: {current_device}\")\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "import data_access.base_loader as base_loader\n",
        "import data_access.ricu_loader as ricu_loader\n",
        "from absl import flags\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "id": "DhiAHsrG6ZaV",
        "outputId": "b7ea37f4-2142-4b8c-ffc1-217beec006ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/icu-autodiff'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "ZRhb2JFH06ef",
        "outputId": "5047336d-b3d7-4f0e-8c4b-02d4fc2e6507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/icu-autodiff/data_access/base_loader.py:596: UserWarning: This will split the data into train, val, test sets stratified on outcome and demographics_to_stratify_on, according to the train_fraction and val_fraction\n",
            "  warnings.warn(\"This will split the data into train, val, test sets stratified on outcome and demographics_to_stratify_on, according to the train_fraction and val_fraction\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapped age groups:\n",
            "{'(0, 30]': 0, '(30, 50]': 1, '(50, 70]': 2, '(70, 100]': 3}\n",
            "Mapped bmi groups:\n",
            "{'(0.0, 18.5]': 0, '(18.5, 24.9]': 1, '(24.9, 29.9]': 2, '(29.9, 100.0]': 3}\n",
            "Saved intermed data!\n",
            "Intermed features path: outputs/mortality24/eicu/intermed/20250527132532/X_20250527132532_intermed.npz\n",
            "Intermed labels path: outputs/mortality24/eicu/intermed/20250527132532/y_20250527132532_intermed.npy\n",
            "Intermed static path: outputs/mortality24/eicu/intermed/20250527132532/c_20250527132532_intermed.npz\n"
          ]
        }
      ],
      "source": [
        "# splitting parameters\n",
        "train_fraction = 0.45\n",
        "val_fraction = 0.1\n",
        "oracle_fraction = 0\n",
        "oracle_min = 100\n",
        "intersectional_min_threshold = 100\n",
        "intersectional_max_threshold = 1000\n",
        "\n",
        "\n",
        "# # data parameters\n",
        "data_name = 'eicu' # 'mimic' 'eicu'\n",
        "task_name = 'mortality24' # 'aki' 'kidney_function' 'los' 'los_24' 'mortality24'\n",
        "static_var = 'ethnicity'\n",
        "features = None\n",
        "ricu_dataset_path = f'raw_data/{task_name}/{data_name}'\n",
        "processed_output_path = f'outputs/{task_name}/{data_name}/processed/'\n",
        "intermed_output_path = f'outputs/{task_name}/{data_name}/intermed/'\n",
        "\n",
        "seed = 0\n",
        "\n",
        "simple_imputation = True\n",
        "mode = 'raw'\n",
        "intermed_data_timestamp = None\n",
        "\n",
        "standardize = False\n",
        "save_intermed_data = True\n",
        "save_processed_data = True\n",
        "\n",
        "\n",
        "split = True # will split into train, val, test, stratified on outcome and demographics_to_stratify_on\n",
        "stratify =  False\n",
        "intersectional = False\n",
        "\n",
        "if split == False:\n",
        "    split_text = 'No Split'\n",
        "\n",
        "'''\n",
        "Two modes of operation:\n",
        "\n",
        "'''\n",
        "\n",
        "loader = ricu_loader.RicuLoader(seed, task_name, data_name,static_var,ricu_dataset_path,simple_imputation,\n",
        "                                    features, processed_output_path,intermed_output_path)\n",
        "\n",
        "\n",
        "if mode == 'raw':\n",
        "    # Create directories if they do not exist\n",
        "    if save_intermed_data:\n",
        "        os.makedirs(intermed_output_path, exist_ok=True)\n",
        "    if save_processed_data:\n",
        "        os.makedirs(processed_output_path, exist_ok=True)\n",
        "\n",
        "    X_dict_tf, y_dict, static = loader.get_data(mode='raw', train_fraction=train_fraction, val_fraction=val_fraction, oracle_fraction=oracle_fraction,\n",
        "                                                oracle_min=oracle_min, intersectional_min_threshold=intersectional_min_threshold,\n",
        "                                                intersectional_max_threshold=intersectional_max_threshold,\n",
        "                                                standardize=standardize,\n",
        "                                                stratify=stratify, intersectional=intersectional, split = split,\n",
        "                                                save_intermed_data=save_intermed_data, save_processed_data=save_processed_data,\n",
        "                                                demographics_to_stratify_on = ['age_group','ethnicity','gender'])\n",
        "else:\n",
        "    raise ValueError(\"Invalid mode specified. Choose 'raw', 'processed', or 'intermediate'.\")\n",
        "\n",
        "if not isinstance(X_dict_tf, dict):\n",
        "    X_dict_tf = {file: X_dict_tf[file] for file in X_dict_tf.files}\n",
        "    y_dict = {file: y_dict[file] for file in y_dict.files}\n",
        "\n",
        "X_dict_tf.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzHu5GCF06ef"
      },
      "source": [
        "# 2) Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "D1PhVWHF06ef"
      },
      "outputs": [],
      "source": [
        "# Add the project root directory to the Python path\n",
        "import sys\n",
        "import os\n",
        "parent_dir = os.path.dirname(os.path.abspath(''))\n",
        "sys.path.append(parent_dir)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import data_access.base_loader as base_loader\n",
        "import data_access.ricu_loader as ricu_loader\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "import ast\n",
        "import logging\n",
        "import json\n",
        "\n",
        "import timeautodiff.processing_simple as processing\n",
        "import timeautodiff.helper_simple as tdf_helper\n",
        "import timeautodiff.timeautodiff_v4_efficient_simple as timeautodiff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOvxdnE306eg"
      },
      "source": [
        "## 2.1) Data Preperation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "7V3-3h9906eg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# most_important_features = [19, 27, 17, 35, 22, 44, 42, 43, 37, 26]\n",
        "X_train = X_dict_tf['X_imputed_train'][:,:,:]\n",
        "X_holdout = X_dict_tf['X_imputed_test'][:,:,:]\n",
        "X_holdout_val = X_dict_tf['X_imputed_val'][:,:,:]\n",
        "\n",
        "m_train = X_dict_tf['m_train'][:,:,:]\n",
        "m_holdout = X_dict_tf['m_test'][:,:,:]\n",
        "m_holdout_val = X_dict_tf['m_val'][:,:,:]\n",
        "\n",
        "feature_names = X_dict_tf['feature_names'][:]\n",
        "y_train = y_dict['y_train'][:]\n",
        "y_holdout = y_dict['y_test'][:]\n",
        "y_holdout_val = y_dict['y_val'][:]\n",
        "\n",
        "\n",
        "static_feature_to_include = ['ethnicity','gender','age_group']\n",
        "static_features_to_include_indices = sorted([y_dict['feature_names'].tolist().index(include)  for include in static_feature_to_include])\n",
        "c_train = y_dict['c_train'][:,static_features_to_include_indices]\n",
        "c_holdout = y_dict['c_test'][:,static_features_to_include_indices]\n",
        "c_holdout_val = y_dict['c_val'][:,static_features_to_include_indices]\n",
        "\n",
        "cond_names = y_dict['feature_names'][static_features_to_include_indices]\n",
        "\n",
        "\n",
        "\n",
        "top10_important_features = [19, 27, 17, 35, 22, 44, 42, 43, 37, 26]\n",
        "top3_important_features = [44,42,43]\n",
        "top6_important_features = [42, 22, 27, 35, 43, 17]\n",
        "\n",
        "important_features_names = X_dict_tf['feature_names'][top10_important_features]\n",
        "important_features_names\n",
        "\n",
        "X_train_10 = processing.normalize_and_reshape(X_train)\n",
        "X_train_10 = X_train_10[:,:,top10_important_features]\n",
        "\n",
        "print('Shape of X train:', X_train.shape)\n",
        "print('Shape of X Holdout:', X_holdout.shape)\n",
        "print('Shape of X Holdout val:', X_holdout_val.shape)\n",
        "\n",
        "print('Shape of y train:', y_train.shape)\n",
        "print('Shape of y Holdout:', y_holdout.shape)\n",
        "print('Shape of y Holdout val:', y_holdout_val.shape)\n",
        "\n",
        "print('Shape of c train:', c_train.shape)\n",
        "print('Shape of c Holdout:', c_holdout.shape)\n",
        "print('Shape of c Holdout val:', c_holdout_val.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "n8ivHYKF06eg"
      },
      "outputs": [],
      "source": [
        "\n",
        "################################################################################################################\n",
        "################################################################################################################\n",
        "################################################################################################################\n",
        "                                                    # Prepare Data for Training #\n",
        "################################################################################################################\n",
        "################################################################################################################\n",
        "################################################################################################################\n",
        "\n",
        "\n",
        "metadata = f\"{data_name}_{task_name}\"\n",
        "\n",
        "process_data = True\n",
        "load_data = False\n",
        "train_models = True\n",
        "train_auto = True\n",
        "train_diff = True\n",
        "load_model = False\n",
        "# processed_data_timestamp = '20241203_130537_10features'\n",
        "\n",
        "model_version = 'v4_efficient_simple'\n",
        "\n",
        "\n",
        "EXP_PATH = os.path.join(os.getcwd(), 'outputs')\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "gen_model = 'TimeAutoDiff'\n",
        "output_dir = f'outputs/{task_name}/{data_name}/{gen_model}/{timestamp}_{len(important_features_names)}features_{model_version}_{metadata}'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "numerical_processing = 'normalize'\n",
        "\n",
        "\n",
        "\n",
        "# prorcess data for training of generators\n",
        "processed_X, processed_y, processed_c, time_info = processing.process_data_for_synthesizer(X_train, y_train, c_train, top10_important_features)\n",
        "cond = torch.concatenate((processed_c, processed_y), axis=2)\n",
        "response = processed_X\n",
        "response = response.float()\n",
        "time_info = time_info.float()\n",
        "\n",
        "\n",
        "metadata = {\n",
        "    'model_version': model_version,\n",
        "    'genmodel_timestamp': timestamp,\n",
        "    'important_features_names': important_features_names.tolist(),\n",
        "    'number of features': len(important_features_names),\n",
        "    'seq_len': processed_X.shape[1],\n",
        "    'seed': seed,\n",
        "    'patient_length': processed_X.shape[0],\n",
        "    'numerical_processing': numerical_processing,\n",
        "}\n",
        "metadata.update(data_params)\n",
        "metadata_path = os.path.join(output_dir, 'metadata.json')\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "\n",
        "\n",
        "################################################################################################################\n",
        "# Checking Processed Data #\n",
        "################################################################################################################\n",
        "\n",
        "print(f\"Shape of the response data: {processed_X.shape}\")\n",
        "print(f\"Shape of the condition data: {cond.shape}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9q8IFIq06eh"
      },
      "source": [
        "## 2.3) Training Auto encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "yZhP3YMz06eh"
      },
      "outputs": [],
      "source": [
        "################################################################################################################\n",
        "################################################################################################################\n",
        "################################################################################################################\n",
        "                                                    # Training #\n",
        "################################################################################################################\n",
        "################################################################################################################\n",
        "################################################################################################################\n",
        "efficient = True\n",
        "auto_mmd_weight = 0\n",
        "auto_consistency_weight = 0\n",
        "diff_mmd_weight = 0\n",
        "diff_consistency_weight = 0\n",
        "full_metadata = f'auto_mmd_{auto_mmd_weight}_auto_cons_{auto_consistency_weight}_diff_mmd_{diff_mmd_weight}_diff_cons_{diff_consistency_weight}'\n",
        "# metadata = f'{id}'\n",
        "\n",
        "use_wandb = False\n",
        "\n",
        "################################################################################################################\n",
        "# Defining Model Parameters #\n",
        "################################################################################################################\n",
        "if train_models:\n",
        "    VAE_training = 200\n",
        "    diff_training = 200\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    ###### Auto-encoder Parameters ######\n",
        "    n_epochs = VAE_training; eps = 1e-5\n",
        "    weight_decay = 1e-6; lr = 2e-4; hidden_size = 200; num_layers = 2; batch_size = 100\n",
        "    channels = 64; min_beta = 1e-5; max_beta = 0.1; emb_dim = 128; time_dim = time_info.shape[2];  lat_dim = response.shape[2]; threshold = 1\n",
        "\n",
        "    if lat_dim > response.shape[2]:\n",
        "        raise ValueError(\"lat_dim should be less than the number of important features.\")\n",
        "\n",
        "    ###### Diffusion Parameters ######\n",
        "    n_epochs = diff_training; hidden_dim = 200; num_layers = 2; diffusion_steps = 100;\n",
        "\n",
        "\n",
        "    new_params = {\n",
        "        \"VAE_training\": VAE_training,\n",
        "        \"diff_training\": diff_training,\n",
        "        \"device\": str(device),\n",
        "        \"imputation strategy\": \"randomly select from imputed patients.\", # \"drop missing values\"\n",
        "        \"eps\" : eps,\n",
        "        \"auto_weight_decay\" : weight_decay,\n",
        "        \"auto_lr\" : lr,\n",
        "        \"auto_hidden_size\" : hidden_size,\n",
        "        \"auto_num_layers\" : num_layers,\n",
        "        \"auto_batch_size\" : batch_size,\n",
        "        \"auto_channels\" : channels,\n",
        "        \"auto_min_beta\" : min_beta,\n",
        "        \"auto_max_beta\" : max_beta,\n",
        "        \"auto_emb_dim\" : emb_dim,\n",
        "        \"auto_time_dim\" : time_dim,\n",
        "        \"auto_lat_dim\" : lat_dim,\n",
        "        \"auto_threshold\" : threshold,\n",
        "        \"diff_hidden_dim\" : hidden_dim,\n",
        "        \"diffusion_steps\" : diffusion_steps,\n",
        "        \"diff_num_layers\" : num_layers,\n",
        "        \"auto_mmd_weight\" : auto_mmd_weight,\n",
        "        \"auto_consistency_weight\" : auto_consistency_weight,\n",
        "        \"diff_mmd_weight\" : diff_mmd_weight,\n",
        "        \"diff_consistency_weight\" : diff_consistency_weight\n",
        "    }\n",
        "\n",
        "    # Call the method\n",
        "    tdf_helper.append_new_params_to_metadata(output_dir, new_params)\n",
        "\n",
        "    # Path to the metadata JSON file\n",
        "    metadata_path = os.path.join(output_dir, 'metadata.json')\n",
        "    # Read the existing JSON file\n",
        "    with open(metadata_path, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    # Extract the parameters\n",
        "    patient_length = metadata.get('patient_length')\n",
        "    imputation_strategy = metadata.get('imputation strategy')\n",
        "    number_of_features = metadata.get('number of features')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ################################################################################################################\n",
        "    # WANDB Initialization #\n",
        "    ################################################################################################################\n",
        "    if use_wandb:\n",
        "        config = dict(\n",
        "            model = \"TimeAutoDiff\",\n",
        "            patient_length = patient_length,\n",
        "            imputation_strategy = imputation_strategy,\n",
        "            number_of_features = number_of_features,\n",
        "            epochs_VAE = VAE_training,\n",
        "            epochs_diffusion = diff_training,\n",
        "            pred_task = task_name,\n",
        "            data_name = data_name,\n",
        "        )\n",
        "\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        wandb.init(\n",
        "            project = 'TimeAutoDiff',\n",
        "            config = config,\n",
        "            name = output_dir.split('/')[-1],\n",
        "        )\n",
        "\n",
        "    ################################################################################################################\n",
        "    # Auto-encoder Training #\n",
        "    ################################################################################################################\n",
        "if train_auto:\n",
        "    torch.cuda.empty_cache()\n",
        "    if efficient:\n",
        "        ds = timeautodiff.train_autoencoder(response, channels, hidden_size, num_layers, lr, weight_decay, n_epochs,\n",
        "                                                      batch_size, min_beta, max_beta, emb_dim, time_dim, lat_dim, device,output_dir, checkpoints=True,\n",
        "                                                    mmd_weight = auto_mmd_weight, consistency_weight = auto_consistency_weight, use_wandb=use_wandb)\n",
        "    # Save Autoencoder\n",
        "    ae = ds[0]\n",
        "    ae.save_model(os.path.join(output_dir, 'autoencoder'))\n",
        "    # Save latent features\n",
        "    latent_features = ds[1]\n",
        "    processing.save_tensor(latent_features,output_dir, 'latent_features.pt')\n",
        "    print(\"Latent features saved successfully.\")\n",
        "else:\n",
        "    latent_features = torch.load(os.path.join(output_dir, 'latent_features.pt'))\n",
        "    ae = timeautodiff.DeapStack.load_model(os.path.join(output_dir, 'autoencoder.pt'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8izD1ah06eh"
      },
      "source": [
        "## 2.3) Training Diffusion Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "y3zTtBJW06eh"
      },
      "outputs": [],
      "source": [
        "################################################################################################################\n",
        "# Diffusion Training #\n",
        "################################################################################################################\n",
        "if train_diff:\n",
        "    num_classes = len(latent_features)\n",
        "\n",
        "    new_params = {\n",
        "        \"diff_num_classes\" : num_classes,\n",
        "    }\n",
        "    # Call the method\n",
        "    tdf_helper.append_new_params_to_metadata(output_dir, new_params)\n",
        "\n",
        "    diff = timeautodiff.train_diffusion(latent_features, cond, time_info, hidden_dim, num_layers, diffusion_steps, n_epochs,output_dir,\n",
        "                                        checkpoints = True, num_classes = num_classes,\n",
        "                                        mmd_weight = diff_mmd_weight, consistency_weight = diff_consistency_weight, use_wandb=use_wandb)\n",
        "with open('output_metadata.txt', 'a') as f:\n",
        "    f.write(f\"Metadata: {metadata}, Full Metadata: {full_metadata}, Output Directory: {output_dir}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kinzgDoX06eh"
      },
      "source": [
        "# 3) Generating Synthetic Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "topGKjKh06eh"
      },
      "source": [
        "## 3.1) Model Loading (In case not loaded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "YAU2ihzr06eh"
      },
      "outputs": [],
      "source": [
        "################################################################################################################\n",
        "# Model Evaluation\n",
        "################################################################################################################\n",
        "output_dir = f'outputs/{task_name}/{data_name}/TimeAutoDiff/'\n",
        "latest_diffusion_timestamp = sorted(os.listdir(output_dir))[-1]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"############ Evaluating timestamp {latest_diffusion_timestamp}: ############\")\n",
        "\n",
        "model = tdf_helper.load_models_only(latest_diffusion_timestamp, task_name, data_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GKkOvKO06ei"
      },
      "source": [
        "## 3.2) Sampling from Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "EiUklq-106ei"
      },
      "outputs": [],
      "source": [
        "response_train, outcome_train, static_train, time_info_train = processing.process_data_for_synthesizer(X_train, y_train, c_train, top10_important_features)\n",
        "cond_train = torch.concatenate((static_train, outcome_train), axis=2)\n",
        "response_train = response_train.float()\n",
        "time_info_train = time_info_train.float()\n",
        "cond_train = cond_train.float()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "bMiSkAC206ei"
      },
      "outputs": [],
      "source": [
        "synth_data_list = []\n",
        "synth_data_y_list = []\n",
        "\n",
        "\n",
        "\n",
        "n_generations = 2\n",
        "for i in tqdm.notebook.tqdm(range(n_generations), desc=\"Generating Synthetic Data\", leave=True):\n",
        "\n",
        "\n",
        "\n",
        "    _synth_data = tdf_helper.generate_synthetic_data_in_batches(model, cond_train, time_info_train,\n",
        "                                                                       batch_size = 10000)\n",
        "    _synth_data_y = cond_train[:, 0, -1]\n",
        "    synth_data_list.append(_synth_data.cpu().numpy())\n",
        "    synth_data_y_list.append(_synth_data_y.cpu().numpy().reshape(-1,))\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}