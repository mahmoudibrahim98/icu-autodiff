{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "import data_access.base_loader as base_loader\n",
    "import data_access.ricu_loader as ricu_loader\n",
    "import os\n",
    "import datetime\n",
    "import wandb\n",
    "import ast\n",
    "import logging\n",
    "import json\n",
    "\n",
    "import timeautodiff.processing_simple as processing\n",
    "import timeautodiff.helper_simple as tdf_helper\n",
    "import timeautodiff.timeautodiff_v4_efficient_simple as timeautodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting parameters\n",
    "train_fraction = 0.2\n",
    "val_fraction = 0.05\n",
    "oracle_fraction = 0\n",
    "oracle_min = 100\n",
    "intersectional_min_threshold = 100\n",
    "intersectional_max_threshold = 1000\n",
    "\n",
    "\n",
    "# # data parameters\n",
    "data_name = 'eicu' # 'mimic' 'eicu'\n",
    "task_name = 'mortality24' # 'aki' 'kidney_function' 'los' 'los_24' 'mortality24' \n",
    "static_var = 'ethnicity'\n",
    "features = None\n",
    "ricu_dataset_path = f'../../real_data/raw/{task_name}/{data_name}'\n",
    "# processed_output_path = f'../../real_data/processed/{task_name}/{data_name}'\n",
    "# intermed_output_path = f'../../real_data/intermed/{task_name}/{data_name}'\n",
    "# processed_data_timestamp = '20250113132215'\n",
    "processed_output_path = f'outputs/{task_name}/{data_name}/processed/'\n",
    "intermed_output_path = f'outputs/{task_name}/{data_name}/intermed/'\n",
    "seed = 0\n",
    "\n",
    "simple_imputation = True\n",
    "mode = 'processed'\n",
    "processed_data_timestamp = '20250429150703'  #'20250429150703'\n",
    "intermed_data_timestamp = None\n",
    "\n",
    "standardize = False\n",
    "save_intermed_data = True\n",
    "save_processed_data = True\n",
    "split = True\n",
    "stratify =  False\n",
    "intersectional = False\n",
    "\n",
    "if split == False:\n",
    "    split_text = 'No Split'\n",
    "else:\n",
    "    split_text = 'Split'\n",
    "data_params = {\n",
    "    'processed_data_timestamp':processed_data_timestamp,\n",
    "    'task_name': task_name,\n",
    "    'data_name': data_name,\n",
    "    'train_fraction': train_fraction,\n",
    "    'val_fraction': val_fraction,\n",
    "    'test_fraction': 1 - train_fraction - val_fraction,\n",
    "    'oracle_fraction': oracle_fraction,\n",
    "    'oracle_min': oracle_min,\n",
    "    'intersectional_min_threshold': intersectional_min_threshold,\n",
    "    'intersectional_max_threshold': intersectional_max_threshold,\n",
    "    'split': split_text,\n",
    "    'standardize' : standardize,\n",
    "}\n",
    "\n",
    "loader = ricu_loader.RicuLoader(seed, task_name, data_name,static_var,ricu_dataset_path,simple_imputation,\n",
    "                                    features, processed_output_path,intermed_output_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_dict_tf, y_dict, static = loader.get_data(\n",
    "    mode='processed', \n",
    "    train_fraction=train_fraction,\n",
    "    val_fraction=val_fraction,\n",
    "    oracle_fraction=oracle_fraction,\n",
    "    oracle_min=oracle_min,\n",
    "    intersectional_min_threshold=intersectional_min_threshold,\n",
    "    intersectional_max_threshold=intersectional_max_threshold,\n",
    "    stratify=stratify,\n",
    "    intersectional=intersectional,\n",
    "    save_intermed_data=False,\n",
    "    save_processed_data=False,\n",
    "    demographics_to_stratify_on = ['age_group','ethnicity','gender'],\n",
    "    processed_timestamp=processed_data_timestamp\n",
    ")\n",
    "    \n",
    "if not isinstance(X_dict_tf, dict):\n",
    "    X_dict_tf = {file: X_dict_tf[file] for file in X_dict_tf.files}\n",
    "    y_dict = {file: y_dict[file] for file in y_dict.files}\n",
    "\n",
    "# data_params = {\n",
    "#     'processed_data_timestamp':processed_data_timestamp,\n",
    "#     'task_name': task_name,\n",
    "#     'data_name': data_name,\n",
    "#     'train_fraction': train_fraction,\n",
    "#     'val_fraction': val_fraction,\n",
    "#     'test_fraction': test_fraction,\n",
    "#     'oracle_fraction': oracle_fraction,\n",
    "#     'oracle_min': oracle_min,\n",
    "#     'intersectional_min_threshold': intersectional_min_threshold,\n",
    "#     'intersectional_max_threshold': intersectional_max_threshold,\n",
    "#     'split': split_text,\n",
    "#     'standardize' : standardize,\n",
    "# }\n",
    "X_dict_tf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# most_important_features = [19, 27, 17, 35, 22, 44, 42, 43, 37, 26]\n",
    "X_train = X_dict_tf['X_imputed_train'][:,:,:]\n",
    "X_test = X_dict_tf['X_imputed_test'][:,:,:]\n",
    "X_val = X_dict_tf['X_imputed_val'][:,:,:]\n",
    "\n",
    "m_train = X_dict_tf['m_train'][:,:,:]\n",
    "m_test = X_dict_tf['m_test'][:,:,:]\n",
    "m_val = X_dict_tf['m_val'][:,:,:]\n",
    "\n",
    "feature_names = X_dict_tf['feature_names'][:]\n",
    "y_train = y_dict['y_train'][:]\n",
    "y_test = y_dict['y_test'][:]\n",
    "y_val = y_dict['y_val'][:]\n",
    "\n",
    "\n",
    "static_feature_to_include = ['ethnicity','gender','age_group']\n",
    "static_features_to_include_indices = sorted([y_dict['feature_names'].tolist().index(include)  for include in static_feature_to_include])\n",
    "c_train = y_dict['c_train'][:,static_features_to_include_indices]\n",
    "c_test = y_dict['c_test'][:,static_features_to_include_indices]\n",
    "c_val = y_dict['c_val'][:,static_features_to_include_indices]\n",
    "\n",
    "cond_names = y_dict['feature_names'][static_features_to_include_indices]\n",
    "\n",
    "# TODO into helpers\n",
    "\n",
    "\n",
    "top10_important_features = [19, 27, 17, 35, 22, 44, 42, 43, 37, 26]\n",
    "top3_important_features = [44,42,43]\n",
    "top6_important_features = [42, 22, 27, 35, 43, 17]\n",
    "\n",
    "important_features_names = X_dict_tf['feature_names'][top10_important_features]\n",
    "important_features_names\n",
    "\n",
    "X_train_10 = processing.normalize_and_reshape(X_train)\n",
    "X_train_10 = X_train_10[:,:,top10_important_features]\n",
    "\n",
    "print('Shape of X train:', X_train.shape)\n",
    "print('Shape of X test:', X_test.shape)\n",
    "print('Shape of X val:', X_val.shape)\n",
    "\n",
    "print('Shape of y train:', y_train.shape)\n",
    "print('Shape of y test:', y_test.shape)\n",
    "print('Shape of y val:', y_val.shape)\n",
    "\n",
    "print('Shape of c train:', c_train.shape)\n",
    "print('Shape of c test:', c_test.shape)\n",
    "print('Shape of c val:', c_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prepeartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "metadata = f\"{data_name}_{task_name}\"\n",
    "\n",
    "process_data = True\n",
    "load_data = False\n",
    "train_models = True\n",
    "train_auto = True\n",
    "train_diff = True\n",
    "load_model = False\n",
    "# processed_data_timestamp = '20241203_130537_10features'\n",
    "\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "                                                    # Prepare Data for Training #\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "model_version = 'v4_efficient_simple'\n",
    "\n",
    "    \n",
    "EXP_PATH = os.path.join(os.getcwd(), 'outputs')\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "gen_model = 'TimeAutoDiff'\n",
    "output_dir = f'outputs/{task_name}/{data_name}/{gen_model}/{timestamp}_{len(important_features_names)}features_{model_version}_{metadata}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "numerical_processing = 'normalize'\n",
    "\n",
    "\n",
    "    \n",
    "# prorcess data for training of generators\n",
    "processed_X, processed_y, processed_c, time_info = processing.process_data_for_synthesizer(X_test, y_test, c_test, top10_important_features)\n",
    "cond = torch.concatenate((processed_c, processed_y), axis=2)\n",
    "response = processed_X\n",
    "response = response.float()\n",
    "time_info = time_info.float()\n",
    "\n",
    "\n",
    "metadata = {\n",
    "    'model_version': model_version,\n",
    "    'genmodel_timestamp': timestamp,\n",
    "    'important_features_names': important_features_names.tolist(),\n",
    "    'number of features': len(important_features_names),\n",
    "    'seq_len': processed_X.shape[1],\n",
    "    'seed': seed,\n",
    "    'patient_length': processed_X.shape[0],\n",
    "    'numerical_processing': numerical_processing,\n",
    "}\n",
    "metadata.update(data_params)\n",
    "metadata_path = os.path.join(output_dir, 'metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "    \n",
    "    \n",
    "################################################################################################################\n",
    "# Checking Processed Data #\n",
    "################################################################################################################\n",
    "\n",
    "print(f\"Shape of the response data: {processed_X.shape}\")\n",
    "print(f\"Shape of the condition data: {cond.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "                                                    # Training #\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "efficient = True\n",
    "auto_mmd_weight = 0\n",
    "auto_consistency_weight = 0\n",
    "diff_mmd_weight = 0\n",
    "diff_consistency_weight = 0\n",
    "full_metadata = f'auto_mmd_{auto_mmd_weight}_auto_cons_{auto_consistency_weight}_diff_mmd_{diff_mmd_weight}_diff_cons_{diff_consistency_weight}'\n",
    "# metadata = f'{id}'\n",
    "\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "# Defining Model Parameters #\n",
    "################################################################################################################\n",
    "if train_models:\n",
    "    VAE_training = 50000\n",
    "    diff_training = 50000\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ###### Auto-encoder Parameters ######\n",
    "    n_epochs = VAE_training; eps = 1e-5\n",
    "    weight_decay = 1e-6; lr = 2e-4; hidden_size = 200; num_layers = 2; batch_size = 100\n",
    "    channels = 64; min_beta = 1e-5; max_beta = 0.1; emb_dim = 128; time_dim = time_info.shape[2];  lat_dim = response.shape[2]; threshold = 1\n",
    "\n",
    "    if lat_dim > response.shape[2]:\n",
    "        raise ValueError(\"lat_dim should be less than the number of important features.\")\n",
    "\n",
    "    ###### Diffusion Parameters ######\n",
    "    n_epochs = diff_training; hidden_dim = 200; num_layers = 2; diffusion_steps = 100;\n",
    "\n",
    "\n",
    "    new_params = {\n",
    "        \"VAE_training\": VAE_training,\n",
    "        \"diff_training\": diff_training,\n",
    "        \"device\": str(device),\n",
    "        \"imputation strategy\": \"randomly select from imputed patients.\", # \"drop missing values\"\n",
    "        \"eps\" : eps,\n",
    "        \"auto_weight_decay\" : weight_decay,\n",
    "        \"auto_lr\" : lr,\n",
    "        \"auto_hidden_size\" : hidden_size,\n",
    "        \"auto_num_layers\" : num_layers,\n",
    "        \"auto_batch_size\" : batch_size,\n",
    "        \"auto_channels\" : channels,\n",
    "        \"auto_min_beta\" : min_beta,\n",
    "        \"auto_max_beta\" : max_beta,\n",
    "        \"auto_emb_dim\" : emb_dim,\n",
    "        \"auto_time_dim\" : time_dim,\n",
    "        \"auto_lat_dim\" : lat_dim,\n",
    "        \"auto_threshold\" : threshold,\n",
    "        \"diff_hidden_dim\" : hidden_dim,\n",
    "        \"diffusion_steps\" : diffusion_steps,\n",
    "        \"diff_num_layers\" : num_layers,\n",
    "        \"auto_mmd_weight\" : auto_mmd_weight,\n",
    "        \"auto_consistency_weight\" : auto_consistency_weight,\n",
    "        \"diff_mmd_weight\" : diff_mmd_weight,\n",
    "        \"diff_consistency_weight\" : diff_consistency_weight    \n",
    "    }   \n",
    "\n",
    "    # Call the method\n",
    "    tdf_helper.append_new_params_to_metadata(output_dir, new_params)\n",
    "\n",
    "    # Path to the metadata JSON file\n",
    "    metadata_path = os.path.join(output_dir, 'metadata.json')\n",
    "    # Read the existing JSON file\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    # Extract the parameters\n",
    "    patient_length = metadata.get('patient_length')\n",
    "    imputation_strategy = metadata.get('imputation strategy')\n",
    "    number_of_features = metadata.get('number of features')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ################################################################################################################\n",
    "    # WANDB Initialization #\n",
    "    ################################################################################################################\n",
    "\n",
    "    config = dict(\n",
    "        model = \"TimeAutoDiff\",\n",
    "        patient_length = patient_length,\n",
    "        imputation_strategy = imputation_strategy,\n",
    "        number_of_features = number_of_features,\n",
    "        epochs_VAE = VAE_training,\n",
    "        epochs_diffusion = diff_training,\n",
    "        pred_task = task_name,\n",
    "        data_name = data_name,\n",
    "    )\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    wandb.init(\n",
    "        project = 'TimeAutoDiff',\n",
    "        config = config,\n",
    "        name = output_dir.split('/')[-1],\n",
    "    )\n",
    "\n",
    "    ################################################################################################################\n",
    "    # Auto-encoder Training #\n",
    "    ################################################################################################################\n",
    "if train_auto:\n",
    "    torch.cuda.empty_cache()\n",
    "    if efficient:\n",
    "        ds = timeautodiff.train_autoencoder(response, channels, hidden_size, num_layers, lr, weight_decay, n_epochs,\n",
    "                                                      batch_size, min_beta, max_beta, emb_dim, time_dim, lat_dim, device,output_dir, checkpoints=True,\n",
    "                                                    mmd_weight = auto_mmd_weight, consistency_weight = auto_consistency_weight)\n",
    "    # Save Autoencoder\n",
    "    ae = ds[0]\n",
    "    ae.save_model(os.path.join(output_dir, 'autoencoder'))\n",
    "    # Save latent features\n",
    "    latent_features = ds[1]\n",
    "    processing.save_tensor(latent_features,output_dir, 'latent_features.pt')\n",
    "    print(\"Latent features saved successfully.\")\n",
    "else:\n",
    "    latent_features = torch.load(os.path.join(output_dir, 'latent_features.pt'))\n",
    "    ae = timeautodiff.DeapStack.load_model(os.path.join(output_dir, 'autoencoder.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "# Diffusion Training #\n",
    "################################################################################################################\n",
    "if train_diff:\n",
    "    num_classes = len(latent_features)\n",
    "\n",
    "    new_params = {\n",
    "        \"diff_num_classes\" : num_classes,\n",
    "    }   \n",
    "    # Call the method\n",
    "    tdf_helper.append_new_params_to_metadata(output_dir, new_params)\n",
    "\n",
    "    diff = timeautodiff.train_diffusion(latent_features, cond, time_info, hidden_dim, num_layers, diffusion_steps, n_epochs,output_dir, checkpoints = True, num_classes = num_classes,\n",
    "                                        mmd_weight = diff_mmd_weight, consistency_weight = diff_consistency_weight)\n",
    "with open('output_metadata.txt', 'a') as f:\n",
    "    f.write(f\"Metadata: {metadata}, Full Metadata: {full_metadata}, Output Directory: {output_dir}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generativeAI_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
