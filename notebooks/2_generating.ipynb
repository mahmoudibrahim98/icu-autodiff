{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch \n",
                "\n",
                "import data_access.base_loader as base_loader\n",
                "import data_access.ricu_loader as ricu_loader\n",
                "import os\n",
                "import datetime\n",
                "import wandb\n",
                "import ast\n",
                "import logging\n",
                "import json\n",
                "\n",
                "import timeautodiff.processing_simple as processing\n",
                "import timeautodiff.helper_simple as tdf_helper\n",
                "import timeautodiff.timeautodiff_v4_efficient_simple as timeautodiff\n",
                "import evaluation_framework.vis as vis\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# splitting parameters\n",
                "train_fraction = 0.2\n",
                "val_fraction = 0.05\n",
                "oracle_fraction = 0\n",
                "oracle_min = 100\n",
                "intersectional_min_threshold = 100\n",
                "intersectional_max_threshold = 1000\n",
                "\n",
                "\n",
                "# # data parameters\n",
                "data_name = 'eicu' # 'mimic' 'eicu'\n",
                "task_name = 'mortality24' # 'aki' 'kidney_function' 'los' 'los_24' 'mortality24' \n",
                "static_var = 'ethnicity'\n",
                "features = None\n",
                "ricu_dataset_path = f'../../real_data/raw/{task_name}/{data_name}'\n",
                "# processed_output_path = f'../../real_data/processed/{task_name}/{data_name}'\n",
                "# intermed_output_path = f'../../real_data/intermed/{task_name}/{data_name}'\n",
                "# processed_data_timestamp = '20250113132215'\n",
                "processed_output_path = f'outputs/{task_name}/{data_name}/processed/'\n",
                "intermed_output_path = f'outputs/{task_name}/{data_name}/intermed/'\n",
                "seed = 0\n",
                "\n",
                "simple_imputation = True\n",
                "mode = 'processed'\n",
                "processed_data_timestamp = '20250501180302'  #'20250501180110'# \n",
                "intermed_data_timestamp = None\n",
                "\n",
                "standardize = False\n",
                "save_intermed_data = True\n",
                "save_processed_data = True\n",
                "split = True\n",
                "stratify =  False\n",
                "intersectional = False\n",
                "\n",
                "if split == False:\n",
                "    split_text = 'No Split'\n",
                "else:\n",
                "    split_text = 'Split'\n",
                "data_params = {\n",
                "    'processed_data_timestamp':processed_data_timestamp,\n",
                "    'task_name': task_name,\n",
                "    'data_name': data_name,\n",
                "    'train_fraction': train_fraction,\n",
                "    'val_fraction': val_fraction,\n",
                "    'test_fraction': 1 - train_fraction - val_fraction,\n",
                "    'oracle_fraction': oracle_fraction,\n",
                "    'oracle_min': oracle_min,\n",
                "    'intersectional_min_threshold': intersectional_min_threshold,\n",
                "    'intersectional_max_threshold': intersectional_max_threshold,\n",
                "    'split': split_text,\n",
                "    'standardize' : standardize,\n",
                "}\n",
                "\n",
                "loader = ricu_loader.RicuLoader(seed, task_name, data_name,static_var,ricu_dataset_path,simple_imputation,\n",
                "                                    features, processed_output_path,intermed_output_path)\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "X_dict_tf, y_dict, static = loader.get_data(\n",
                "    mode='processed', \n",
                "    train_fraction=train_fraction,\n",
                "    val_fraction=val_fraction,\n",
                "    oracle_fraction=oracle_fraction,\n",
                "    oracle_min=oracle_min,\n",
                "    intersectional_min_threshold=intersectional_min_threshold,\n",
                "    intersectional_max_threshold=intersectional_max_threshold,\n",
                "    stratify=stratify,\n",
                "    intersectional=intersectional,\n",
                "    save_intermed_data=False,\n",
                "    save_processed_data=False,\n",
                "    demographics_to_stratify_on = ['age_group','ethnicity','gender'],\n",
                "    processed_timestamp=processed_data_timestamp\n",
                ")\n",
                "    \n",
                "if not isinstance(X_dict_tf, dict):\n",
                "    X_dict_tf = {file: X_dict_tf[file] for file in X_dict_tf.files}\n",
                "    y_dict = {file: y_dict[file] for file in y_dict.files}\n",
                "\n",
                "# data_params = {\n",
                "#     'processed_data_timestamp':processed_data_timestamp,\n",
                "#     'task_name': task_name,\n",
                "#     'data_name': data_name,\n",
                "#     'train_fraction': train_fraction,\n",
                "#     'val_fraction': val_fraction,\n",
                "#     'test_fraction': test_fraction,\n",
                "#     'oracle_fraction': oracle_fraction,\n",
                "#     'oracle_min': oracle_min,\n",
                "#     'intersectional_min_threshold': intersectional_min_threshold,\n",
                "#     'intersectional_max_threshold': intersectional_max_threshold,\n",
                "#     'split': split_text,\n",
                "#     'standardize' : standardize,\n",
                "# }\n",
                "X_dict_tf.keys()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# most_important_features = [19, 27, 17, 35, 22, 44, 42, 43, 37, 26]\n",
                "X_train = X_dict_tf['X_imputed_train'][:,:,:]\n",
                "X_test = X_dict_tf['X_imputed_test'][:,:,:]\n",
                "X_val = X_dict_tf['X_imputed_val'][:,:,:]\n",
                "\n",
                "m_train = X_dict_tf['m_train'][:,:,:]\n",
                "m_test = X_dict_tf['m_test'][:,:,:]\n",
                "m_val = X_dict_tf['m_val'][:,:,:]\n",
                "\n",
                "feature_names = X_dict_tf['feature_names'][:]\n",
                "y_train = y_dict['y_train'][:]\n",
                "y_test = y_dict['y_test'][:]\n",
                "y_val = y_dict['y_val'][:]\n",
                "\n",
                "\n",
                "static_feature_names = ['ethnicity','gender','age_group']\n",
                "static_features_to_include_indices = sorted([y_dict['feature_names'].tolist().index(include)  for include in static_feature_names])\n",
                "c_train = y_dict['c_train'][:,static_features_to_include_indices]\n",
                "c_test = y_dict['c_test'][:,static_features_to_include_indices]\n",
                "c_val = y_dict['c_val'][:,static_features_to_include_indices]\n",
                "\n",
                "cond_names = static_feature_names\n",
                "\n",
                "\n",
                "top10_important_features = [19, 27, 17, 35, 22, 44, 42, 43, 37, 26]\n",
                "top3_important_features = [44,42,43]\n",
                "top6_important_features = [42, 22, 27, 35, 43, 17]\n",
                "\n",
                "important_features_names = X_dict_tf['feature_names'][top10_important_features]\n",
                "important_features_names\n",
                "\n",
                "X_train_10 = processing.normalize_and_reshape(X_train)\n",
                "X_train_10 = X_train_10[:,:,top10_important_features]\n",
                "y_train_10 = y_train\n",
                "\n",
                "X_val_10 = processing.normalize_and_reshape(X_val)\n",
                "X_val_10 = X_val_10[:,:,top10_important_features]\n",
                "y_val_10 = y_val\n",
                "\n",
                "print('Shape of X train:', X_train.shape)\n",
                "print('Shape of X test:', X_test.shape)\n",
                "print('Shape of X val:', X_val.shape)\n",
                "\n",
                "print('Shape of y train:', y_train.shape)\n",
                "print('Shape of y test:', y_test.shape)\n",
                "print('Shape of y val:', y_val.shape)\n",
                "\n",
                "print('Shape of c train:', c_train.shape)\n",
                "print('Shape of c test:', c_test.shape)\n",
                "print('Shape of c val:', c_val.shape)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "################################################################################################################\n",
                "# Model Evaluation\n",
                "################################################################################################################\n",
                "################################################################################################################\n",
                "# Model Evaluation\n",
                "################################################################################################################\n",
                "diff_timestamp = '20250430_124212_10features_v4_efficient_simple_eicu_mortality24'\n",
                "diff_timestamp = diff_timestamps[0]\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"############ Evaluating timestamp {diff_timestamp}: ############\")\n",
                "\n",
                "model = tdf_helper.load_models_only(diff_timestamp, task_name, data_name)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "response_test, outcome_test, static_test, time_info_test = processing.process_data_for_synthesizer(X_test, y_test, c_test, top10_important_features)\n",
                "cond_test = torch.concatenate((static_test, outcome_test), axis=2)\n",
                "response_test = response_test.float()\n",
                "time_info_test = time_info_test.float()\n",
                "cond_test = cond_test.float()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Sampling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "synth_data_list = []\n",
                "synth_data_y_list = []\n",
                "\n",
                "# Generate synthetic data\n",
                "\n",
                "\n",
                "# # for test data \n",
                "# real_data_y = conditioning_test[:, 0, outcome_indices]\n",
                "# _synth_data_y = conditioning_test[:, 0, outcome_indices]\n",
                "# demographic_data = conditioning_test[:, 0, cond_indices]\n",
                "\n",
                "sample_set = 'test'\n",
                "hybrid = False\n",
                "\n",
                "if sample_set == 'test':\n",
                "    conditioning = cond_test\n",
                "    time = time_info_test\n",
                "    real = response_test\n",
                "# elif sample_set == 'oracle':\n",
                "#     conditioning = conditioning_oracle\n",
                "#     time = time_oracle\n",
                "#     real = real_oracle\n",
                "# if hybrid:\n",
                "#     _, complementary_real ,_, complmentary_conditioning= train_test_split(real_test,conditioning_test, test_size = real_test.shape[0] - real_oracle.shape[0] , stratify = conditioning_test[:,0,cond_indices].cpu().numpy() )\n",
                "#     hybrid_conditioning = torch.cat((conditioning_oracle, complmentary_conditioning), dim=0)\n",
                "#     hybrid_demographic_data = hybrid_conditioning[:,0,cond_indices]\n",
                "\n",
                "\n",
                "real_data_y = conditioning[:, 0, -1]\n",
                "_synth_data_y = conditioning[:, 0, -1]\n",
                "demographic_data = conditioning[:, 0, :-1]\n",
                "\n",
                "# remove single sample subgroups from data\n",
                "mask_test = single_sample_subgroups_mask(demographic_data.cpu().numpy())\n",
                "real_data_y = real_data_y[mask_test]\n",
                "_synth_data_y = _synth_data_y[mask_test]\n",
                "demographic_data = demographic_data[mask_test]\n",
                "conditioning = conditioning[mask_test]\n",
                "time = time[mask_test]\n",
                "real = real[mask_test]\n",
                "\n",
                "# # remove single sample subgroups from train data\n",
                "# mask_train = single_sample_subgroups_mask(c_train)\n",
                "# c_train = c_train[mask_train]\n",
                "# down_train_X = down_train_X[mask_train]\n",
                "# down_train_y = down_train_y[mask_train]\n",
                "n_generations = 2\n",
                "for i in tqdm(range(n_generations), desc=\"Generating Synthetic Data\", leave=True):\n",
                "\n",
                "\n",
                "\n",
                "    _synth_data = tdf_helper.generate_synthetic_data_in_batches(model, cond_test, time_info_test, \n",
                "                                                                       batch_size = 10000)\n",
                "    \n",
                "    if not hybrid:\n",
                "        synth_data_list.append(_synth_data.cpu().numpy())\n",
                "        synth_data_y_list.append(_synth_data_y.cpu().numpy().reshape(-1,))\n",
                "    # else:\n",
                "    #     # create hybrid data consisting of oracle synthetic ata and remaining real data from test set\n",
                "    #     hybrid_data = torch.cat((_synth_data, complementary_real.cpu()), dim=0)\n",
                "    #     hybrid_y = hybrid_conditioning[:,0,outcome_indices]\n",
                "    #     synth_data_list.append(hybrid_data.cpu().numpy())\n",
                "    #     synth_data_y_list.append(hybrid_y.cpu().numpy().reshape(-1,))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "generativeAI_clone",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
